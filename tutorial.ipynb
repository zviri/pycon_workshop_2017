{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Machine learning with text in Scikit-learn\n",
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction to supervised learning in scikit-learn](#Introduction-to-supervised-learning-in-scikit-learn)\n",
    "1. [Converting text to feature vectors](#From-text-to-feature-vectors)\n",
    "1. [Classifying creditors from the Czech Insolvency Register](#Classifying-creditors-from-the-Czech-Insolvency-Register)\n",
    "    1. [Loading and preprocessing the dataset](#Loading-and-preprocessing-the-dataset)\n",
    "    1. [Vectorizing the dataset](#Vectorizing-the-dataset)\n",
    "    1. [Building and evaluating the model](#Building-and-evaluating-the-model)\n",
    "    1. [Examining the model](#Examining-the-model)\n",
    "1. [Topics not covered](#Topics-not-covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for Python 2 users\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to supervised learning in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\">Wikipedia</a>:**<br>\n",
    "**Supervised learning** is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value.\n",
    "\n",
    "**Note:** We will consider a classification task, i.e., samples belong to two or more classes that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset.\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data      # feature matrix, on sample per row\n",
    "y = iris.target    # target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the shapes of X and y.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nicer overview of our dataset.\n",
    "dataset = pd.DataFrame(X, columns=iris.feature_names)\n",
    "dataset[\"label\"] = y\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the target vector\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init logistic regression model with default params.\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the model. \n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a sample from the training data.\n",
    "s = X[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try to predict the target value for this sample.\n",
    "clf.predict([s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In scikit-learn, an estimator for classification is a Python object that implements the methods **fit(X, y)** and **predict(samples)**. \n",
    "\n",
    "**To summarize the general process:**\n",
    "1. Get a dataset in form **X** (feature matrix) and **y** (target variable)\n",
    "2. Pick a model and fit it using **.fit(X, y)**\n",
    "3. Predict values of new, unobserved samples using **.predict(samples)**\n",
    "\n",
    "You can also check the basic introduction to ML with scikit-learn in the <a href=\"http://scikit-learn.org/stable/tutorial/basic/tutorial.html\">documentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From text to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dataset = [\"A coward judges all he sees by what he is.\",\n",
    "                \"There are people who need people to need them.\",\n",
    "                \"Never's the word God listens for when he needs a laugh.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <a target=\"_blank\" href=\"http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\">scikit-learn documentation</a>:<br>\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, **a sequence of symbols cannot be fed directly to the algorithms themselves** as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <a target=\"_blank\" href=\"http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\">scikit-learn documentation</a>:<br>\n",
    "\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into **numerical feature vectors**.\n",
    "\n",
    "The most intuitive way to do so is the **bags of words** representation:\n",
    "1. assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "1. for each document **#i**, count the number of occurrences of each word w and store it in **X[i, j]** as the value of feature **#j** where **j** is the index of word **w** in the dictionary\n",
    "\n",
    "The bags of words representation implies that **n_features** is the number of distinct words in the corpus: **this number is typically larger than 100,000**.\n",
    "\n",
    "Fortunately, **most values in X will be zeros** since for a given document less than a couple thousands of distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "\n",
    "We will use scikits **CountVectorizer** to convert text into a **matrix of token counts (document-term matrix)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init CountVectorizer with the default params.\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn the vocabulary from the text data.\n",
    "vectorizer.fit(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine the vocabulary.\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary size: {0}\".format(len(vocabulary)))\n",
    "print(\"Vocabulary:\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform text data into a document-term matrix.\n",
    "dtm = vectorizer.transform(text_dataset)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the obtained document-term matrix.\n",
    "pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**<br>\n",
    "**Vectorization** is a general process of turning a collection of text documents into numerical feature vectors.<br>\n",
    "**CountVectorizer** is one of the vectorizers available in scikit-learn.<br>\n",
    "All vectorizers are used as follows:\n",
    "* use **.fit(data)** to learn the vocabulary\n",
    "* use **.transform(data)** to build the document-term matrix from text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying creditors from the Czech Insolvency Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset with pandas.\n",
    "dataset = pd.read_table(\"./data/receivables.tsv\", encoding=\"utf-8\", header=0)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the number of samples per class.\n",
    "dataset.groupby('creditor').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since we have texts written in Czech in the dataset, let's remove the accents (diacritics) from the text first.\n",
    "def remove_accents(s):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', s)\n",
    "    ascii_string = nkfd_form.encode('ASCII', 'ignore')\n",
    "    return ascii_string\n",
    "\n",
    "dataset[\"text\"] = dataset[\"text\"].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dataset without accents again.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scikit-learn required numerical values as labels, so let's convert \n",
    "# the creditors' names to numbers first using LabelEncoder.\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(dataset['creditor'])\n",
    "numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the classes.\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage of label_encoder.\n",
    "label_encoder.transform([\"CSOB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the numberic_labels to the dataset so that we have everything in one place.\n",
    "dataset[\"numeric_label\"] = numeric_labels\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the feature vectors and target variables.\n",
    "# The feature vector still contains just raw texts.\n",
    "X = dataset[\"text\"]\n",
    "y = dataset[\"numeric_label\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the prepared list of stopwords for the Czech language.\n",
    "stopwords = pd.read_csv('data/stopwords_cz.txt', encoding='utf-8', header=None, names=[\"word\"])\n",
    "stopwords[\"word\"] = stopwords[\"word\"].apply(remove_accents)\n",
    "print(\"Number of stopwords: {0}\".format(len(stopwords)))\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer, this time with customized params.\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             ngram_range=(1,3),\n",
    "                             stop_words=list(stopwords[\"word\"].values),\n",
    "                             max_df = 0.5,\n",
    "                             min_df = 30,\n",
    "                             tokenizer = lambda x: re.split(\"[\\r\\t\\n .,;:'\\\"()?!/]+\", x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "* **lowercase** - convert all characters to lowercase before tokenizing.\n",
    "* **ngram_range** - the lower and upper boundary of the range of n-values for different n-grams to be extracted. \n",
    "* **stop_words** - list of stopwords which will be removed from the vocabulary.\n",
    "* **max_df** - ignore terms that have a document frequency strictly higher than the this threshold (float from [0.0, 1.0] for relative value or integer for absolute value).\n",
    "* **min_df** - ignore terms that have a document frequency strictly lower than the this threshold (float from [0.0, 1.0] for relative value or integer for absolute value).\n",
    "* **tokenizer** - used to specify a custom tokenization (i.e. splitting text to words) step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn the vocabulary and check its size.\n",
    "vectorizer.fit(X_train)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform train data into a document-term matrix.\n",
    "X_train_dtm = vectorizer.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform test data into a document-term matrix.\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init logistic regression model, this time with slightly changed params.\n",
    "clf = LogisticRegression(C=1.0, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model and time it with IPython magic command.\n",
    "%time clf.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test data.\n",
    "y_predictions = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy of your predictions.\n",
    "metrics.accuracy_score(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix.\n",
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_predictions), \n",
    "             index=label_encoder.classes_, columns=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the model\n",
    "Can we somehow find out what has the model actually learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the logistic regressions params.\n",
    "print(clf.coef_.shape)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of feature names and classes.\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "classes = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a redable version of our model and check its learned parameters.\n",
    "readable_model = pd.DataFrame(clf.coef_.transpose(), columns=classes)\n",
    "readable_model.insert(0, \"ngram\", feature_names)\n",
    "readable_model[7000:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine parameters for specific class.\n",
    "readable_model.sort_values(by=\"Ceska sporitelna\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has the model learned? Check\n",
    "<a target=\"_blank\" href=\"https://www.google.cz/maps/@50.0448653,14.4483488,3a,75y,195.68h,98.1t/data=!3m6!1e1!3m4!1s0K232cRC0B4x6gwNN8GG7Q!2e0!7i13312!8i6656!6m1!1e1?hl=en\">Olbrachtova 1929, Praha 4</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics not covered\n",
    "* Advanced text preprocessing.\n",
    "* Better model evaluation.\n",
    "* Comparison of different models.\n",
    "* (Meta) Parameter optimization of each model.\n",
    "* ... and much much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
